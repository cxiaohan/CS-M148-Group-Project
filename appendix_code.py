# -*- coding: utf-8 -*-
"""Appendix_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fg0pzGaKt6P49_jmBlaW30DS_jN2VZxb

# i) EDA
"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

# Import Dataset
spotify = pd.read_csv("hf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv")

spotify.columns

spotify.shape

spotify.head(20)

# drop the repeated index col
spotify = spotify.drop(columns=['Unnamed: 0'])

# Checking for null values
na_num = spotify.isnull().sum()
print("Null values: " )
print(na_num)

na_rows = spotify[spotify.isnull().any(axis=1)]
print(na_rows)

# Since only 1 out of 114000 data points has null values, we will drop the row
spotify = spotify.dropna()

# Checking for duplicates
has_dup = spotify['track_id'].duplicated().value_counts()
has_dup

spotify = spotify.drop_duplicates(subset=['track_id'], keep='first')
spotify.shape

# Description of the Data
spotify.describe()

# Boxplot
# find the number of numeric columns
num_col = len(spotify.select_dtypes(include='number').columns)

# set up for the plots
plots_per_row = 4
num_rows = (num_col + plots_per_row - 1) // plots_per_row

# boxplot for each numeric column
spotify.select_dtypes(include='number').plot(
    kind='box',
    subplots=True,
    layout=(num_rows, plots_per_row),  # Create multiple rows
    figsize=(15, num_rows * 5),  # Adjust height for the number of rows
    grid=True
)

# show the boxplots
plt.tight_layout()
plt.show()

# Histogram
# find numeric columns
numeric_cols = spotify.select_dtypes(include='number').columns

# set up for the plots
plots_per_row = 4
num_rows = (num_col + plots_per_row - 1) // plots_per_row

# Create histograms for each numeric column
plt.figure(figsize=(15, num_rows * 5))

i = 1
for col in numeric_cols:
    plt.subplot(num_rows, plots_per_row, i)  # Create a subplot for each column
    sns.histplot(spotify[col], bins=30, kde=True)
    plt.title(f'Histogram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')
    i += 1

plt.tight_layout()  # Adjust layout
plt.show()

# Bar plot with selected columns

plt.figure(figsize=(15, 10))

# Bar plot for 'explicit'
plt.subplot(2, 2, 1)
sns.countplot(data=spotify, x='explicit')
plt.title('Explicit Tracks Count')
plt.xlabel('Explicit')
plt.ylabel('Count')

# Bar plot for 'key'
plt.subplot(2, 2, 2)
sns.countplot(data=spotify, x='key')
plt.title('Tracks by Key Count')
plt.xlabel('Key')
plt.ylabel('Count')

# Bar plot for 'mode'
plt.subplot(2, 2, 3)
sns.countplot(data=spotify, x='mode')
plt.title('Tracks by Mode Count')
plt.xlabel('Mode (0=Minor, 1=Major)')
plt.ylabel('Count')

# Bar plot for 'time_signature'
plt.subplot(2, 2, 4)
sns.countplot(data=spotify, x='time_signature')
plt.title('Tracks by Time Signature Count')
plt.xlabel('Time Signature')
plt.ylabel('Count')

plt.tight_layout()
plt.show()

# Total songs per genre
spotify_grouped = spotify.groupby('track_genre', as_index=False).count().sort_values(by='track_id', ascending=False)

# Extracting the data
y = spotify_grouped['track_genre']
x = spotify_grouped['track_id']

# Bar chart
fig, ax = plt.subplots(figsize=(12, 16))
sns.barplot(x=x, y=y, ax=ax)
ax.set_title("Total Songs Based on Genres", fontsize=16, fontweight='bold', loc='center')
ax.set_ylabel("Genre", fontsize=12)
ax.set_xlabel("Total Songs", fontsize=12)

# Adjust layout
plt.tight_layout()
plt.show()

# correlation matrix
correlation_matrix = spotify.select_dtypes(include='number').corr()

# heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f", cmap='coolwarm', square=True, cbar_kws={"shrink": .8})

plt.title('Correlation Matrix Heatmap')
plt.tight_layout()

"""# ii) Data Pre-processing"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import StandardScaler
pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

"""## Data Loading and Structure"""

spotify = pd.read_csv("hf://datasets/maharshipandya/spotify-tracks-dataset/dataset.csv")

spotify.columns

# drop the repeated index col
spotify = spotify.drop(columns=['Unnamed: 0'])

spotify.shape

spotify.head(20)

spotify.select_dtypes('number').describe()

"""## Data Cleaning

### Drop NA values
"""

# drop NA values
na_num = spotify.isnull().sum()
na_num
na_rows = spotify[spotify.isnull().any(axis=1)]
na_rows

# check if all NA values are dropped
missing_prop = na_num / len(spotify.index)
missing_prop.sort_values()

# visualiza if NA still exist
px.imshow(spotify.isna().astype(int), color_continuous_scale='Greys')

# since there is only one row contain NA, and the missing value is such a small portion of our data
# I would drop the NA value
spotify_valid = spotify.dropna()
has_dup = spotify_valid['track_id'].duplicated().any()
print(has_dup)

"""### Drop duplicate values"""

# drop duplicate values
spotify_unique = spotify_valid.drop_duplicates(subset=['track_id'], keep=False)

spotify_unique.shape

non_num_cols = spotify_unique.select_dtypes(exclude=['number'])
non_num_cols.head()
for col in non_num_cols.columns:
    print(f"Unique values in column '{col}':")
    print(spotify_unique[col].unique())
    print("\n")

spotify_clean = spotify_unique.copy()
# factorize the column track_genre
codes, uniques = pd.factorize(spotify_clean['track_genre'])

# add the factorized values as a new column in the DataFrame (optional)
spotify_clean['track_genre'] = codes

# display the factorized column and the unique values
print(spotify_clean['track_genre'])

"""### Handle categorical features (Explicity, Key, Time Signature)
- one hot encoding: Explicit
- target encoding: track_genre (there are too many genres, so we chose target encoding instead of one-hot encoding which would result in too many features), key, time_signature
"""

# one hot coding for explicity
mapping = {False: 0, True: 1}
spotify_clean['explicit'] = spotify_clean['explicit'].map(mapping)

# Display the first few rows to check the result
spotify_clean.head()

# one-hot encoding for 'key' and 'time_signature'
key_encoded = pd.get_dummies(spotify_clean['key'], prefix='key')
time_signature_encoded = pd.get_dummies(spotify_clean['time_signature'], prefix='time_signature')

spotify_clean = pd.concat([spotify_clean, key_encoded, time_signature_encoded], axis=1)
spotify_clean = spotify_clean.drop(columns=['key', 'time_signature'])

# convert boolean to num
spotify_clean.iloc[:, -17:] = spotify_clean.iloc[:, -17:].astype(int)

"""### Handle numeric features
- standardize: duration_ms, loudness, tempo

"""

# standardize necessary features
standard_scaler = StandardScaler()
standard_scaler_features = ['duration_ms', 'loudness', 'tempo']
spotify_clean[standard_scaler_features] = standard_scaler.fit_transform(spotify_clean[standard_scaler_features])

spotify_clean.describe()

# compute the mean popularity for each genre
genre_target_mean = spotify_clean.groupby('track_genre')['popularity'].mean()
# map the mean popularity to the genre
spotify_clean['track_genre'] = spotify_clean['track_genre'].map(genre_target_mean)

spotify_clean.head()

# save the DataFrame to a CSV file
spotify_clean.to_csv('spotify_clean.csv', index=False)

"""# iii) Linear Regression"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

spotify = pd.read_csv('spotify_clean.csv')

spotify.head()

from sklearn.linear_model import Ridge
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

# Choose energy as the response variable and tempo as the predictor variable
# Choose popularity as the response variable and tempo as the predictor variable
train_df, val_df = train_test_split(spotify, test_size=0.3, random_state=42)
potential_features = train_df[['duration_ms','danceability','energy','loudness','speechiness','acousticness','instrumentalness','liveness','valence','tempo']]

# Since we are predicting popularity, we need to figure out what variable to use
# as a predictor.

# 1. Calculate the correlation between the popularity variable
#   and the set of remaining potential predictor variables
correlations = []
for column in potential_features.columns:
  correlation = spotify['popularity'].corr(spotify[column])
  correlations.append((column, correlation))
correlations.sort(key=lambda x: x[1], reverse=True)

# 2. Visualize the correlations between the dependent variable
px.bar(x=[x[0] for x in correlations], y=[x[1] for x in correlations],
       title = 'Correlation Between Popularity and Potential Predictor Variables',
       labels={'x': 'Potential Predictors',
               'y':'Correlation with Popularity'})

"""Form the plot, we observe that 'instrumentalness', 'danceability', and 'loudness' have the highest correlation with popularity (by absolute value). Hence we use it as the predictor. However, note that the absolute values of correlations are quite low, at a range of 0.05-0.15."""

# Commented out IPython magic to ensure Python compatibility.
# %pip install scikit-lego
from sklearn.linear_model import LinearRegression
from sklego.linear_model import LADRegression

# First use all continuous featurs to perform multivariable LS and LAD regression
# Fit multivariable LS regression
X_train = train_df[['instrumentalness', 'danceability', 'loudness']]
y_train = train_df['popularity']
X_test = val_df[['instrumentalness', 'danceability', 'loudness']]
y_test = val_df['popularity']

ls_spotify = LinearRegression()
ls_spotify.fit(X_train, y_train)

# Fit a LAD regression model
lad_spotify = LADRegression()
lad_spotify.fit(X_train,y_train)

import matplotlib.pyplot as plt
import numpy as np

# To visualize the modles, plot instrumentalness vs. popularity and regression lines
plt.figure(figsize=(12, 6))
plt.scatter(train_df['instrumentalness'], train_df['popularity'], s=10, alpha=0.7, label='Data Points')

# LAD regression line (dashed)
lad_y = lad_spotify.intercept_ + train_df['instrumentalness'] * lad_spotify.coef_[0]
plt.plot(train_df['instrumentalness'], lad_y, linestyle='--', color='black', label='LAD', linewidth=2)

# LS regression line (solid)
ls_y = ls_spotify.intercept_ + train_df['instrumentalness'] * ls_spotify.coef_[0]
plt.plot(train_df['instrumentalness'], ls_y, linestyle='-', color='blue', label='LS', linewidth=2)

plt.title("Scatter Plot with Regression Lines", fontsize=14)
plt.xlabel("Instrumentalness", fontsize=12)
plt.ylabel("Popularity", fontsize=12)
plt.legend(fontsize=10)
plt.show()

"""From the plot, it is very obvious that instrumentalness does not make reasonable predictions of popularity, even if it has the highest correlation with it. However we still calculate evaluation metrics to confirm this understanding."""

# create a dataframe of true values and predicted values from the regression models (training set)
pred_train_df = pd.DataFrame(
    {'true': train_df['popularity'],
     'ls_pred': ls_spotify.predict(X_train),
     'lad_pred': lad_spotify.predict(X_train)})
pred_train_df

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
# compute the rMSE, MAE, MAD, correlation and R2 of the true price with the LS and LAD predictions (training set)
print('LS rMSE:', np.sqrt(mean_squared_error(pred_train_df['true'], pred_train_df['ls_pred'])))
print('LS MAE:', mean_absolute_error(pred_train_df['true'], pred_train_df['ls_pred']))
print('LS MAD:', np.median(np.abs(pred_train_df['true'] - pred_train_df['ls_pred'])))
print('LS correlation:', np.corrcoef(pred_train_df['true'], pred_train_df['ls_pred'])[0, 1])
print('LS R2:', r2_score(pred_train_df['true'], pred_train_df['ls_pred']))

print('LAD rMSE:', np.sqrt(mean_squared_error(pred_train_df['true'], pred_train_df['lad_pred'])))
print('LAD MAE:', mean_absolute_error(pred_train_df['true'], pred_train_df['lad_pred']))
print('LAD MAD:', np.median(np.abs(pred_train_df['true'] - pred_train_df['lad_pred'])))
print('LAD correlation:', np.corrcoef(pred_train_df['true'], pred_train_df['lad_pred'])[0, 1])
print('LAD R2:', r2_score(pred_train_df['true'], pred_train_df['lad_pred']))

# create a dataframe of true values and predicted values from the regression models (validation set)
pred_val_df = pd.DataFrame(
    {'true': val_df['popularity'],
     'ls_pred': ls_spotify.predict(X_test),
     'lad_pred': lad_spotify.predict(X_test)})
pred_val_df

# compute the rMSE, MAE, MAD, correlation and R2 of the true price with the LS and LAD predictions (validation set)
print('LS rMSE:', np.sqrt(mean_squared_error(pred_val_df['true'], pred_val_df['ls_pred'])))
print('LS MAE:', mean_absolute_error(pred_val_df['true'], pred_val_df['ls_pred']))
print('LS MAD:', np.median(np.abs(pred_val_df['true'] - pred_val_df['ls_pred'])))
print('LS correlation:', np.corrcoef(pred_val_df['true'], pred_val_df['ls_pred'])[0, 1])
print('LS R2:', r2_score(pred_val_df['true'], pred_val_df['ls_pred']))

print('LAD rMSE:', np.sqrt(mean_squared_error(pred_val_df['true'], pred_val_df['lad_pred'])))
print('LAD MAE:', mean_absolute_error(pred_val_df['true'], pred_val_df['lad_pred']))
print('LAD MAD:', np.median(np.abs(pred_val_df['true'] - pred_val_df['lad_pred'])))
print('LAD correlation:', np.corrcoef(pred_val_df['true'], pred_val_df['lad_pred'])[0, 1])
print('LAD R2:', r2_score(pred_val_df['true'], pred_val_df['lad_pred']))

"""The metrics performed quite poorly with a correlations at about 0.15-0.16 and R^2 values at around 0.01-0.02. As a result, we revert to the full dataset and apply regularization to perform feature selection in hopes of improving model performance."""

# Prepare variable for doing Ridge and Lasso Regression
# Use only the numerical features of the dataset for regression
X_train_reg = train_df.iloc[:,5:]

# Scale the predictors
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import  Ridge, Lasso

scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train_reg)
y_train = train_df['popularity']

# scale the predictorsfrom sklearn.linear_model import  Ridge, Lasso
from sklearn.model_selection import cross_val_score, cross_validate

# use 10-fold cross-validation to select the best lambda (alpha) value for the ridge regression model
# define the alpha values to test
alphas = np.logspace(-1, 6, 100)

ridge_cv_scores = []
# create a for loop to compute the cross-validation score for each alpha value
for alpha in alphas:
    ridge = Ridge(alpha=alpha)
    ridge_cv = cross_validate(estimator=ridge,
                              X=X_train_std,
                              y=y_train,
                              cv=10,
                              scoring='neg_root_mean_squared_error')
    ridge_cv_scores.append({'alpha': alpha,
                            'log_alpha': np.log(alpha),
                            'test_mse': -np.mean(ridge_cv['test_score'])})

# convert the cross-validation scores into a data frame
ridge_cv_scores_df = pd.DataFrame(ridge_cv_scores)

# plot the cross-validation scores as a function of alpha
px.line(ridge_cv_scores_df,
        x='log_alpha',
        y='test_mse',
        title='Ridge')

# use 10-fold cross-validation to select the best lambda (alpha) value for the lasso regression model
# define the alpha values to test
alphas = np.logspace(-1, 4, 100)

# create an empty list to store the cross-validation scores
lasso_cv_scores = []

# create a for loop to compute the cross-validation score for each alpha value
for alpha in alphas:
    lasso = Lasso(alpha=alpha)
    lasso_cv = cross_validate(estimator=lasso,
                              X=X_train_std,
                              y=y_train,
                              cv=10,
                              scoring='neg_root_mean_squared_error')
    lasso_cv_scores.append({'alpha': alpha,
                            'log_alpha': np.log(alpha),
                            'test_mse': -np.mean(lasso_cv['test_score'])})

# convert the cross-validation scores into a data frame
lasso_cv_scores_df = pd.DataFrame(lasso_cv_scores)

# plot the cross-validation scores as a function of alpha
px.line(lasso_cv_scores_df,
        x='log_alpha',
        y='test_mse',
        title='Lasso')

# identify the value of alpha that minimizes the cross-validation score for ridge
ridge_alpha_min = ridge_cv_scores_df.sort_values(by='test_mse').head(1).alpha.values[0]
# compute the min MSE and the SE of the MSE
mse_se_ridge = ridge_cv_scores_df['test_mse'].std() / np.sqrt(10)
mse_min_ridge = ridge_cv_scores_df['test_mse'].min()

# identify the value of alpha that minimizes the cross-validation score for ridge within 1SE
ridge_alpha_1se = ridge_cv_scores_df[(ridge_cv_scores_df['test_mse'] <= mse_min_ridge + mse_se_ridge) &
                                     (ridge_cv_scores_df['test_mse'] >= mse_min_ridge - mse_se_ridge)].sort_values(by='alpha', ascending=False).head(1).alpha.values[0]

# identify the value of alpha that minimizes the cross-validation score for lasso
lasso_alpha_min = lasso_cv_scores_df.sort_values(by='test_mse').head(1).alpha.values[0]
# compute the min MSE and the SE of the MSE
mse_se_lasso = lasso_cv_scores_df['test_mse'].std() / np.sqrt(10)
mse_min_lasso = lasso_cv_scores_df['test_mse'].min()

# identify the value of alpha that minimizes the cross-validation score for lasso within 1SE
lasso_alpha_1se = lasso_cv_scores_df[(lasso_cv_scores_df['test_mse'] <= mse_min_lasso + mse_se_lasso) &
                                     (lasso_cv_scores_df['test_mse'] >= mse_min_lasso - mse_se_lasso)].sort_values(by='alpha', ascending=False).head(1).alpha.values[0]

print('Ridge (min): ', ridge_alpha_min)
print('Ridge (1SE): ', ridge_alpha_1se)
print('Lasso (min): ', lasso_alpha_min)
print('Lasso (1SE): ', lasso_alpha_1se)

# use ridge_alpha_min to fit the ridge regression model (training set)
ridge_min_fit = Ridge(alpha=ridge_alpha_min).fit(X=X_train_std, y=y_train)
ridge_1se_fit = Ridge(alpha=ridge_alpha_1se).fit(X=X_train_std, y=y_train)

# use lasso_alpha_min to fit the lasso regression model
lasso_min_fit = Lasso(alpha=lasso_alpha_min).fit(X=X_train_std, y=y_train)
lasso_1se_fit = Lasso(alpha=lasso_alpha_1se).fit(X=X_train_std, y=y_train)

# Predictions using the ridge models (training set)
ridge_min_predictions = ridge_min_fit.predict(X_train_std)  # Predict using ridge model with alpha_min
ridge_1se_predictions = ridge_1se_fit.predict(X_train_std)  # Predict using ridge model with alpha_1se

# Predictions using the lasso models (training set)
lasso_min_predictions = lasso_min_fit.predict(X_train_std)  # Predict using lasso model with alpha_min
lasso_1se_predictions = lasso_1se_fit.predict(X_train_std)

from sklearn.metrics import mean_squared_error, r2_score

# Evaluate metric using the fitted models after applying regularization (training set)

# For Ridge models

ridge_min_rmse = np.sqrt(mean_squared_error(y_train, ridge_min_predictions))
ridge_min_r2 = r2_score(y_train, ridge_min_predictions)
ridge_min_corr = np.corrcoef(y_train, ridge_min_predictions)[0, 1]

ridge_1se_rmse = np.sqrt(mean_squared_error(y_train, ridge_1se_predictions))
ridge_1se_r2 = r2_score(y_train, ridge_1se_predictions)
ridge_1se_corr = np.corrcoef(y_train, ridge_1se_predictions)[0, 1]

# For Lasso models
lasso_min_rmse = np.sqrt(mean_squared_error(y_train, lasso_min_predictions))
lasso_min_r2 = r2_score(y_train, lasso_min_predictions)
lasso_min_corr = np.corrcoef(y_train, lasso_min_predictions)[0, 1]

lasso_1se_rmse = np.sqrt(mean_squared_error(y_train, lasso_1se_predictions))
lasso_1se_r2 = r2_score(y_train, lasso_1se_predictions)
lasso_1se_corr = np.corrcoef(y_train, lasso_1se_predictions)[0, 1]

# Print out the results
print(f'Ridge alpha_min rMSE: {ridge_min_rmse}, R²: {ridge_min_r2}, correlation: {ridge_min_corr}')
print(f'Ridge alpha_1se rMSE: {ridge_1se_rmse}, R²: {ridge_1se_r2}, correlation: {ridge_1se_corr}')
print(f'Lasso alpha_min rMSE: {lasso_min_rmse}, R²: {lasso_min_r2}, correlation: {lasso_min_corr}')
print(f'Lasso alpha_1se rMSE: {lasso_1se_rmse}, R²: {lasso_1se_r2}, correlation: {lasso_1se_corr}')

# Prepare variable for doing Ridge and Lasso Regression
# Use only the numerical features of the dataset for regression
X_val_reg = val_df.iloc[:,5:]

# Scale the predictors
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import  Ridge, Lasso

scaler = StandardScaler()
X_val_std = scaler.fit_transform(X_val_reg)
y_val = val_df['popularity']

# use ridge_alpha_min to fit the ridge regression model (validation set)
ridge_min_fit = Ridge(alpha=ridge_alpha_min).fit(X=X_val_reg, y=y_val)
ridge_1se_fit = Ridge(alpha=ridge_alpha_1se).fit(X=X_val_reg, y=y_val)

# use lasso_alpha_min to fit the lasso regression model (validation set)
lasso_min_fit = Lasso(alpha=lasso_alpha_min).fit(X=X_val_reg, y=y_val)
lasso_1se_fit = Lasso(alpha=lasso_alpha_1se).fit(X=X_val_reg, y=y_val)

# Predictions using the ridge models (validation set)
ridge_min_predictions = ridge_min_fit.predict(X_val_reg)  # Predict using ridge model with alpha_min
ridge_1se_predictions = ridge_1se_fit.predict(X_val_reg)  # Predict using ridge model with alpha_1se

# Predictions using the lasso models (validation set)
lasso_min_predictions = lasso_min_fit.predict(X_val_reg)  # Predict using lasso model with alpha_min
lasso_1se_predictions = lasso_1se_fit.predict(X_val_reg)

# Evaluate metric using the fitted models after applying regularization (validation set)

# For Ridge models

ridge_min_rmse = np.sqrt(mean_squared_error(y_val, ridge_min_predictions))
ridge_min_r2 = r2_score(y_val, ridge_min_predictions)

ridge_1se_rmse = np.sqrt(mean_squared_error(y_val, ridge_1se_predictions))
ridge_1se_r2 = r2_score(y_val, ridge_1se_predictions)

# For Lasso models
lasso_min_rmse = np.sqrt(mean_squared_error(y_val, lasso_min_predictions))
lasso_min_r2 = r2_score(y_val, lasso_min_predictions)

lasso_1se_rmse = np.sqrt(mean_squared_error(y_val, lasso_1se_predictions))
lasso_1se_r2 = r2_score(y_val, lasso_1se_predictions)

# Print out the results
print(f'Ridge alpha_min rMSE: {ridge_min_rmse}, R²: {ridge_min_r2}, correlation: {ridge_min_corr}')
print(f'Ridge alpha_1se rMSE: {ridge_1se_rmse}, R²: {ridge_1se_r2}, correlation: {ridge_1se_corr}')
print(f'Lasso alpha_min rMSE: {lasso_min_rmse}, R²: {lasso_min_r2}, correlation: {lasso_min_corr}')
print(f'Lasso alpha_1se rMSE: {lasso_1se_rmse}, R²: {lasso_1se_r2}, correlation: {lasso_1se_corr}')

# Pair feature names with Ridge regression coefficients
ridge_coefficients = pd.DataFrame({
    'Feature': X_train_reg.columns,
    'Ridge Coefficient': ridge_min_fit.coef_
})


# Pair feature names with Lasso regression coefficients
lasso_coefficients = pd.DataFrame({
    'Feature': X_train_reg.columns,
    'Lasso Coefficient': lasso_min_fit.coef_
})

# Print Ridge coefficients by absolute magnitude
ridge_coefficients_sorted = ridge_coefficients.reindex(ridge_coefficients['Ridge Coefficient'].abs().sort_values(ascending=False).index)
print("Ridge Coefficients (Sorted by Magnitude):")
print(ridge_coefficients_sorted)

# Print Lasso coefficients by absolute magnitude
lasso_coefficients_sorted = lasso_coefficients.reindex(lasso_coefficients['Lasso Coefficient'].abs().sort_values(ascending=False).index)
print("Lasso Coefficients (Sorted by Magnitude):")
print(lasso_coefficients_sorted)

"""# iv) Logistic Regression is discussed in Main Document Notebook

# v) KNN
"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score, classification_report
from sklearn.model_selection import KFold, cross_val_score
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.ensemble import RandomForestClassifier

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

spotify = pd.read_csv('spotify_clean.csv')

popularity_threshold = 35
spotify['popularity_binary'] = spotify['popularity'].apply(lambda x: 1 if x > popularity_threshold else 0)

print(spotify['popularity_binary'].value_counts(normalize=True))

X = spotify[['danceability', 'energy', 'speechiness', 'acousticness',
                    'valence', 'instrumentalness']]
y = spotify['popularity_binary']

# split data, test-train-val in 75%-15%-10%
X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.25, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.4, random_state=42)

# Import necessary libraries
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, f1_score

# Create a KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)  # n_neighbors is the number of neighbors to consider

# Train the classifier
knn.fit(X_train, y_train)

# Make predictions on the training set
y_train_pred = knn.predict(X_train)

# confusion matrix
conf_matrix = confusion_matrix(y_train, y_train_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_train, y_train_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_train, y_train_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

# Make predictions on the test set
y_test_pred = knn.predict(X_test)

# confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_test, y_test_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_test, y_test_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

# ROC Curve
y_prob = knn.predict_proba(X_val)[:, 1]
fpr, tpr, thresholds = roc_curve(y_val, y_prob)
roc_auc = roc_auc_score(y_val, y_prob)

# Plot ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

# 5-fold CV
cross_val_score(knn, X_val, y_val, cv=5, scoring='roc_auc') # auc score

from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
from sklearn import metrics

# Use the shuffle and random state if want data shuffled before splitting
skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
i = 1
for train_index, test_index in skfolds.split(X, y):
    clone_knn = clone(knn)
    X_train_folds = X.iloc[train_index]
    y_train_folds = y.iloc[train_index]
    X_test_fold = X.iloc[test_index]
    print(test_index)
    clone_knn.fit(X_train_folds, y_train_folds)
    y_pred = clone_knn.predict(X_test_fold)

    auc_sample = metrics.roc_auc_score(y.iloc[test_index], y_pred)
    print('Fold: ', i)
    print('AUC: ', auc_sample)
    print('Accuracy: ', metrics.accuracy_score(y.iloc[test_index], y_pred))
    i += 1

# Train a decision tree on the same data to compare results
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a decision tree classifier
tree = DecisionTreeClassifier(max_depth=6, random_state=42)
tree.fit(X_train, y_train)

y_train_pred = tree.predict(X_train)

# confusion matrix (training set)
conf_matrix = confusion_matrix(y_train, y_train_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics (training set)
accuracy = accuracy_score(y_train, y_train_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_train, y_train_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

# test decision tree performance on test set
y_test_pred = tree.predict(X_test)

# confusion matrix (training set)
conf_matrix = confusion_matrix(y_test, y_test_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics (training set)
accuracy = accuracy_score(y_test, y_test_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_test, y_test_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

# Train a Random Forest Classifier with 100 trees
rf = RandomForestClassifier(n_estimators=100, max_depth=6, random_state=42)
rf.fit(X_train, y_train)

# Predict on training data
y_train_pred = rf.predict(X_train)

# confusion matrix
conf_matrix = confusion_matrix(y_train, y_train_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_train, y_train_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_train, y_train_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

# Test random forest performance on testing data
# Predict on test data
y_test_pred = rf.predict(X_test)

# confusion matrix
conf_matrix = confusion_matrix(y_test, y_test_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_test, y_test_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)
f1 = f1_score(y_test, y_test_pred)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")
print("F1 Score:", f1)

"""# vi) PCA and Clusterings"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

from random import sample

import plotly.graph_objects as go
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score, silhouette_samples, rand_score, adjusted_rand_score
from itertools import product
from sklearn.decomposition import PCA


from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split

spotify=pd.read_csv("spotify_clean.csv")

spotify.head()

spotify.shape

# standardize num values
ss = StandardScaler()
spotify_ss = ss.fit_transform(spotify.drop(spotify.columns[:5],axis=1))

# perform PCA
pca = PCA()
X_pca = pca.fit_transform(spotify_ss)

# Plot explained variance ratio
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')
plt.title('Explained Variance Ratio by Principal Component')
plt.xlabel('Principal Component')
plt.ylabel('Explained Variance Ratio')
plt.grid()
plt.show()

# select top components that explain most of the variance
n_components = 5
pca = PCA(n_components=n_components)
X_pca_reduced = pca.fit_transform(spotify_ss)

# randomly sample 5000 points from the dataset since the datset is really big for h-clustering
sample_size = 5000
if X_pca_reduced.shape[0] > sample_size:
    sampled_indices = np.random.choice(X_pca_reduced.shape[0], sample_size, replace=False)
    X_sampled = X_pca_reduced[sampled_indices]
else:
    X_sampled = X_pca_reduced

# define the range of k values to test
k_values = range(1, 11)

# list to store the within-cluster sum of squares (WCSS)
wcss = []

# loop through the range of k values
for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_sampled)
    wcss.append(kmeans.inertia_)

# elbow curve
plt.figure(figsize=(8, 6))
plt.plot(k_values, wcss, marker='o', linestyle='-', color='b')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.grid(True)
plt.show()

"""Since the turning point for the elbow method is around 3, I choose 3 as the numbe of clusters."""

kmeans = KMeans(n_clusters=3, random_state=10)
kmeans_labels = kmeans.fit_predict(X_sampled)

plt.figure(figsize=(10, 7))
scatter = plt.scatter(X_sampled[:, 0], X_sampled[:, 1], c=kmeans_labels, cmap='viridis', s=50, alpha=0.8)
plt.colorbar(scatter, label="Cluster Labels")
plt.title("KMeans Clustering Visualization (PCA Reduced Data)")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.grid(True)
plt.show()

# perform hierarchical clustering on the sampled data
linkage_matrix = linkage(X_sampled, method='ward')

from sklearn.metrics import silhouette_score, adjusted_rand_score
from sklearn.metrics.cluster import rand_score
import numpy as np

# generate hierarchical clustering labels
num_clusters = 3
from scipy.cluster.hierarchy import fcluster
hierarchical_labels = fcluster(linkage_matrix, num_clusters, criterion='maxclust')

# Calculate the metrics for the two clustering methods
# Silhouette Score
silhouette_hierarchical = silhouette_score(X_sampled, hierarchical_labels)
print(f"Silhouette Score (Hierarchical): {silhouette_hierarchical}")
silhouette_k = silhouette_score(X_sampled, kmeans_labels)
print(f"Silhouette Score (K-means): {silhouette_k}")

# Rand Index and Adjusted Rand Index for comparing with K-means
rand_index = rand_score(kmeans_labels, hierarchical_labels)
adjusted_rand_index = adjusted_rand_score(kmeans_labels, hierarchical_labels)

print(f"Rand Index (Hierarchical vs K-means): {rand_index}")
print(f"Adjusted Rand Index (Hierarchical vs K-means): {adjusted_rand_index}")

# Total Within-Sum-of-Squares (WSS)
def tot_within_sum_of_square(data, labels):
    wss = 0
    for cluster_label in np.unique(labels):
        cluster_points = data[labels == cluster_label]
        cluster_center = cluster_points.mean(axis=0)
        wss += np.sum((cluster_points - cluster_center) ** 2)
    return wss

wss_hierarchical = tot_within_sum_of_square(X_sampled, hierarchical_labels)
print(f"Total WSS (Hierarchical): {wss_hierarchical}")

wss_k = tot_within_sum_of_square(X_sampled, kmeans_labels)
print(f"Total WSS (Hierarchical): {wss_k}")

import matplotlib.pyplot as plt
import numpy as np

# Generate the scatter plot of the clusters
plt.figure(figsize=(10, 7))
for cluster in np.unique(hierarchical_labels):
    cluster_points = X_sampled[hierarchical_labels == cluster]
    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f"Cluster {cluster}", alpha=0.7)

plt.title("Hierarchical Clustering Results")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.show()

data_with_clusters = pd.DataFrame(X_sampled, columns=["PC1", "PC2","PC3", "PC4","PC5"])  # add PCA data
data_with_clusters["Cluster"] = hierarchical_labels  # add cluster labels
data_with_clusters["Performance"] = spotify["popularity"]  # add performance variable
# summary statistics for performance in each cluster
cluster_performance = data_with_clusters.groupby("Cluster")["Performance"].describe()
print(cluster_performance)

"""# vii) Neural Network"""

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

from google.colab import drive
drive.mount("/content/drive", force_remount=True)




import pandas as pd
spotify = pd.read_csv('/content/drive/My Drive/CSM148_Final_Proj/spotify_clean.csv')

spotify.head()

import pandas as pd
spotify = pd.read_csv('/content/drive/My Drive/CSM148_Final_Proj/spotify_clean.csv')
# Define two levels
spotify['popularity_level'] = pd.cut(
    spotify['popularity'],
    bins=[-1, 30, 100],  # Bins for Low (<30), High (>30)
    labels=[0, 1]
)

# Select features and target
features = [
    "danceability", "energy", "valence", "tempo", "acousticness",
    "instrumentalness", "liveness", "speechiness", "loudness",
    "track_genre"
]
target = "popularity_level"


spotify = spotify.dropna(subset=features + [target])
scaler = StandardScaler()
X = scaler.fit_transform(spotify[features])

y = spotify[target].values

X_tensor = torch.tensor(X, dtype=torch.float32)
y_tensor = torch.tensor(y, dtype=torch.long)

X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=42)

# Create DataLoader for batching
train_dataset = TensorDataset(X_train, y_train)
test_dataset = TensorDataset(X_test, y_test)
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt

# Define the Neural Network model
class NeuralNetwork(nn.Module):
    def __init__(self, input_size, num_classes):
        super(NeuralNetwork, self).__init__()
        self.fc1 = nn.Linear(input_size, 64)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.relu(x)
        x = self.fc3(x)
        return x

# Function to train and evaluate the model for a given learning rate
def train_and_evaluate_model(learning_rate):
    model = NeuralNetwork(input_size, num_classes)

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Train the model
    epochs = 40
    train_losses = []
    model.train()
    for epoch in range(epochs):
        epoch_loss = 0
        for batch in train_loader:
            X_batch, y_batch = batch

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward pass
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        train_losses.append(epoch_loss / len(train_loader))

    # Evaluate the model
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for X_batch, y_batch in test_loader:
            outputs = model(X_batch)
            _, predicted = torch.max(outputs, 1)
            total += y_batch.size(0)
            correct += (predicted == y_batch).sum().item()

    accuracy = 100 * correct / total
    return accuracy, train_losses

# Hyperparameter tuning: Test multiple learning rates
learning_rates = [0.001, 0.01, 0.1]
best_accuracy = 0
best_learning_rate = None
all_results = {}

for lr in learning_rates:
    print(f"Testing learning rate: {lr}")
    accuracy, train_losses = train_and_evaluate_model(lr)
    all_results[lr] = (accuracy, train_losses)
    print(f"Learning Rate: {lr}, Test Accuracy: {accuracy:.2f}%")
    if accuracy > best_accuracy:
        best_accuracy = accuracy
        best_learning_rate = lr

print(f"Best Learning Rate: {best_learning_rate}, Best Accuracy: {best_accuracy:.2f}%")

# Plot training losses for all learning rates
for lr, (accuracy, train_losses) in all_results.items():
    plt.plot(train_losses, label=f"LR: {lr}, Acc: {accuracy:.2f}%")

plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Training Loss Over Epochs for Different Learning Rates")
plt.legend()
plt.show()

"""# viii) Hyperparameters Tuning is discussed in Appendix text and code is included above"""