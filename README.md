# CS-M148-Group-Project
Group project for team Data Scholars
Team: Data Scholars
Members: Xiaohan Sun, Jiayue Liu, Eliza Jiang, Ruoming Wu, Jianong Xu

## Main document: 

**i. the data set your team used**

We used the Spotify Prediction Data Set that was provided through our class on Bruinlearn. This dataset contains general metadata about Spotify tracks, alongside audio features and a popularity score for each track on its platform on a scale of 0-100. The data is in csv format.

**ii. the overview of the problem your team addressed**

The problem we aimed to address was the ability to classify the popularity of future songs or songs not included in the current dataset by training on the provided data. Popularity is a key metric for platforms like Spotify, as it directly influences decisions on what songs to promote, feature, and potentially monetize. It serves as a meaningful label that provides insights into user preferences, platform trends, and revenue potential.

Our objective was to develop a binary classification model that predicts whether a track will be popular or unpopular based on its audio features and metadata. By leveraging the variables in the dataset, we sought to train machine learning models capable of accurately classifying the popularity of new tracks. This task not only addresses a practical challenge for music streaming services but also provides valuable insights into the relationship between a track's attributes and its success on the platform.

**iii. the key methodology that worked to address the problem with explanations as to why**

The primary methodology we employed for this problem was a logistic regression model, approached as a binary classification task. We defined the popularity threshold as 50, based on both the dataset's distribution and our domain intuition. Songs were classified as popular if they had an original popularity score greater than 50 and as unpopular otherwise.

The features included in the model were a combination of original numeric variables and encoded categorical variables. The target variable was a binary feature on popularity, where 0 represented "not popular" (popularity ≤ 50) and 1 represented "popular" (popularity > 50).
We divided the dataset into training, testing, and validation subsets. The training data was used to fit the model, the testing data to evaluate performance, and the validation data for cross-validation to ensure robustness. The model achieved a testing accuracy of approximately 72%, which matched the training accuracy. This consistency indicates that the model neither overfits nor underfits and performs well on unseen data. Additionally, the model's ROC curve and AUC score (~0.81) further demonstrated its strong predictive performance.

There are a few reasons behind our choice of logistic regression as key methodology. First, the problem was defined as a binary classification task, and logistic regression directly models the probability of the binary outcomes, making it an inherently suitable methodology. Additionally, logistic regression is computationally efficient, making it ideal for our large dataset size. It allowed us to quickly iterate through model adjustments and allowed for a streamlined modeling process. Finally, logistic regression provides interpretable coefficients, which could help us understand the relationship between the predictors and the target variable, such as which features contributed positively or negatively to a song's popularity.

For further details on the implementation of the logistic regression model, please refer to Appendix IV in this document.

**iv. results including cross-validation used and evaluation metrics and conclusions such as why you chose the key method and its limitations**

A 5-fold cross-validation with shuffled splits was performed, with accuracy scores across folds ranging from 71.31% to 72.71% and AUC scores ranging from 0.8029 to 0.8190. These metrics confirm the model's robustness across different data splits, reflects strong discriminatory power,  and indicates that the performance is consistent. The ROC and AUC metrics also show that the model balances sensitivity and specificity effectively. As mentioned above, both training and testing sets yielded an accuracy of around 72% and prediction error around 28%. This consistency indicates that the model generalizes well and avoids overfitting or underfitting.

From the logistic regression analysis, we examined the coefficients and learned that features like ‘explicit’, ‘danceability’, and ‘track_genre’ contribute positively to the popularity because they have positive coefficients. The most negatively influential features are ‘time_signature_0,’ ‘speechiness,’ and ‘liveness’. Also, we learned that the dataset was not balanced in terms of popularity as it has fewer popular tracks compared to non-popular tracks. We were able to use this analysis for feature importance using the magnitude and sign of the model’s coefficients.

We selected logistic regression as the primary model for several reasons: First, logistic is a powerful and interpretable baseline model for binary classification tasks and well-suited for our problem. Second, a preceding linear regression analysis indicated that the data did not exhibit a strictly linear relationship, necessitating a model like logistic regression that incorporates non-linearity through transformation. Finally, among all models tested—including linear regression, logistic regression, K-nearest neighbors, K-means clustering, and neural networks—logistic regression achieved the highest accuracy with the lowest error rate, making the most reliable and accurate prediction choice for our task.

However, although having a moderately high accuracy and true positive/negative rates, the true positive rate is always a little higher than the true negative rate. This suggests that the model tends to perform better at identifying popular tracks but has limitations in predicting unpopular ones, which requires potential future refinement on the representation of our classed data. Additionally, logistic regression assumes a linear relationship between features and the log-odds of the target, which may require additional feature engineering such as creating interaction terms to further enhance our predictions. Another limitation associated with our model is that while it performed well for this dataset, it may not scale efficiently with larger datasets or higher-dimensional features. Advanced models like neural networks could be explored more to address such scenarios.

**v. how to use the code for your project on the data set.**

Please run the EDA section of the codebook for preliminary visualizations and analyses of the dataset. In the Data Preprocessing section, we addressed missing values, removed duplicates, standardized numeric features, and target-encoded the categorical feature “track_genre” by mapping it to its genre-averaged popularity score.

Our Logistic Regression module provides a step-by-step implementation of the training, validation, and testing processes for our main binary classification model. This section includes detailed code and visualizations, as well as an examination of the model's result coefficients.
Additionally, other sections document the implementation and results of the alternative models we explored, including their comparative performance. For further discussions and supplementary information, please refer to the Appendix.


## Appendix: 

**i. Explain the exploratory data analysis that you conducted. What was done to visualize your data and split your data for training and testing?**

We began the EDA by inspecting the dataset to understand its structure and content. Specifically, we examined the dataset’s columns, shape, and the first few rows to get a general sense of the data types and values present. We identified and handled missing values by removing them because of their extremely low proportion, and addressed duplicate rows to ensure a clean and meaningful dataset for analysis. Next, we applied descriptive statistics to examine summary metrics such as mean, median, standard deviation, and range, which provided insights into the variability and central tendencies of the numerical features.

To visualize the data, we employed a variety of plots. Box plots were used to detect and visualize potential outliers in the numerical variables, while histograms helped us understand the distribution of these variables, highlighting features that exhibited skewness or non-uniform distributions. For some chosen categorical variables, we used bar plots to explore their distribution, as histograms are less effective for visualizing non-numeric data. We then analyzed the total number of songs in each genre to determine whether genre could serve as a useful predictor in subsequent analyses. We also constructed a heatmap based on the correlation matrix to analyze the relationships between features. This visualization allowed us to identify highly correlated features, which can inform feature selection and avoid redundancy in the predictive modeling process.

The EDA provided essential information for splitting the data into training and testing sets. With over 80,000 data points, we concluded that assigning a relatively larger portion of the data to the testing set would not compromise the robustness of the training algorithm. Additionally, we noted that some variables, such as liveness and speechiness, exhibited skewed distributions. This observation highlighted the importance of ensuring that the training and testing sets maintain similar feature distributions to accurately represent the original dataset. 

**ii. What data pre-processing and feature engineering (or data augmentation) did you complete on your project?**
 
We first examined the dataset for missing values and identified one track with missing information. Given the dataset's size of 114,000 tracks, removing this single track had very little impact on its completeness and quality. For the categorical feature with two categories ‘explicit’, we applied one-hot encoding, converting it into a binary numeric variable. For categorical features with multiple categories, such as ‘key’ and ‘time_signature’, we also applied one-hot encoding. For the categorical feature ‘track_genre’, we used target encoding. This approach was chosen to handle the multiple levels of these features, preventing unnecessarily increasing the dimensionality of the dataset. Numeric features, including duration_ms, loudness, and tempo, were standardized to ensure that they are on comparable scales.

**iii. How was regression analysis applied to your project? What did you learn about your dataset from this analysis and were you able to use this analysis for feature importance? Was regularization needed?**

To apply linear regression in predicting popularity, we first found the correlation of popularity with all features with continuous numerical values, which allowed us to conclude that ‘instrumentalness’ had the highest correlation with popularity. Then we fitted LS and LAD regression models using ‘instrumentalness’ as the predictor variable. However, the resulting models performed poorly with low values of R^2 (around 0.02) and correlation coefficient (around 0.16). In an attempt to mitigate the issue, we then used ridge and lasso regression to train multivariate regression models that included all features of continuous values. We applied 10-fold cross validation to select the optimal lambda value for both lasso and ridge regression. Then, the resulting models were applied to the training set to test for performance. There were significant improvements in both R^2 and correlation, with the new R^2 being approximately 0.4 and correlation being approximately 0.63. When applied to the testing set, the values of both metrics were similar, also showing significant improvement from the non-cross-validated model. Hence regularization was needed to obtain a better performance.

From this analysis, we were able to learn more about the distribution of popularity. Specifically, when plotting popularity against instrumentalness in our first regression model (without regularization), we could observe a non-linear relationship between the two variables. It seemed that popularity tended to cluster around both ends of the scale for instrumentalness. Considering that instrumentalness already had the strongest correlation with popularity, this result suggests that linear regression may not be the best method for making predictions. However, with the help of regularization, we were able to standardize the variables and use the magnitude of their coefficients to identify feature importance. In ridge regression, danceability and speechiness were identified as the most important predictors, and in lasso regression, track_genre and time_signature_4 were the most important predictors.

**iv. How was logistic regression analysis applied in your project? What did you learn about your data set from this analysis and were you able to use this analysis for feature importance? Was regularization needed?**

Logistic regression was applied to our project to predict whether a song is popular or not with a pre-defined popularity threshold, equivalent to a binary classification problem. Based on the popularity distribution and our intuition, we chose the popularity threshold as 50, so any song with popularity above 50 would be classified as popular. This threshold is a reasonable choice because it is the natural midpoint of a 0-100 score scale, and it contains interpretability for people to understand a song’s popularity. The features we included for logistic regression are original numeric features and encoded categorical features, including ‘duration_ms’, ‘explicit’, ‘danceability’, ‘energy’, ‘loudness’, ‘mode’, ‘speechiness’, ‘acousticness’, ‘instrumentalness’, ‘liveness’, ‘valence’, ‘tempo’, ‘track_genre’, encoded ‘key’, and encoded ‘time signature’. The target variable is an encoded binary variable ‘popularity_binary’ with 0 representing not popular (<= popularity threshold) and 1 representing popular (> popularity threshold). We splitted dataset into train, test, and validation, using training data to train the model, testing data to evaluate model performance, and validation to perform cross validation to evaluate the model. The testing accuracy is around 72% which is the same as our training accuracy, implying that the model is not overfitting or underfitting and performs well on unseen data.

From the logistic regression analysis, we examined the coefficients and learned that features like ‘explicit’, ‘danceability’, and ‘track genre’ contribute positively to the popularity because they have positive coefficients. The most negatively influential features are ‘time_signature_0,’ ‘speechiness,’ and ‘liveness’. Also, we learned that the dataset was not balanced in terms of popularity as it has fewer popular tracks compared to non-popular tracks. We were able to use this analysis for feature importance using the magnitude and sign of the model’s coefficients. Since the model accuracy on the training set and testing set are similar, there is no sign of overfitting, so regularization was not needed in this case. This also indicates effective feature engineering applied, making the original dataset cleaner. 

**v. How were KNN, decision trees, or random forest used for classification on your data? What method worked best for your data and why was it good for the problem you were addressing?**

To apply KNN, decision trees, and random forest to predict popularity, we first converted the continuous popularity variable into a binary variable, separated by a threshold of 35. This threshold creates a fairly even split of all samples. Then, KNN was applied to this binary-converted dataset to predict the class of popularity that each training sample belongs to by using the KNeighborsClassifier from sklearn. Similarly, the DecisionTreeClassifier and RandomForestClassifier were used to implement the decision tree and random forest algorithms. KNN yielded accuracies of 0.75 (training) and 0.63 (testing), decision tree yielded accuracies of 0.63 (training) and 0.61 (testing), and random forest yielded accuracies of 0.64 (training) and 0.63 (testing). Out of all three models, the decision tree had the highest true positive and true negative rates, as well as the highest f1 score, indicating a better performance in balancing precision and recall. Based on this result, the decision tree worked best for classifying each sample to their appropriate popularity class.

The decision tree was a good method for the problem we were addressing because, from previous analysis, we could tell that popularity follows a distribution that is not highly correlated with any single feature. The decision tree algorithm evaluates all possible splits across multiple features and selects the splits that minimize the variance of the two popularity classes at each node. This process allows the model to reveal relationships that are not immediately apparent in the raw data, making it effective for classification tasks with weakly correlated predictors. Additionally, the decision tree is generally more computationally efficient than both KNN and random forest, making it more ideal to work with given our large dataset.

**vi. How were PCA and clustering applied on your data? What method worked best for your data and why was it good for the problem you were addressing?**

PCA was implemented before K-means and hierarchical clustering. We first standardized our dataset without our response variable, which is ‘popularity’ in this case. Then, the original standardized dataset went through the dimension reduction process of PCA. From the scree plot, we can see decreasing returns in variability explained by additional components after the fifth PCs. Therefore, taking the first 5 PCs corresponds to a reasonable summary of the dataset. 
Since the spotify dataset is very large, in order to conduct clustering analysis, we took 5000 samples after PCA. From elbow’s method, since k = 3 is at the turning point, we used 3 as the number of clusters for both clustering methods. The result shows that both clusters have silhouette scores below 0.31, showing a relatively poor clustering fit, although the two methods seem to agree with each other with Rand Index around 0.804. Overall, hierarchical clustering performed better with a slightly higher silhouette score of 0.308 (compared to 0.267), and lower WSS of 28120.85 (compared to 29555.40).

After applying PCA, the clusterings do show there seem to exist a certain pattern in our dataset with distinct clusters. However, the summary of the hierarchical clusters has showed that the ‘popularity’ scores of each cluster does not vary much with mean between 33-34. Also, since the goal of our project is to predict the level of popularity, clusterings only give us a pattern of the data without a clear prediction, meaning clusterings were not that helpful to us compared to other models and algorithms. 

**vii. Explain how your project attempted to use a neural network on the data and the results of that attempt.**

In this project, a neural network was designed to classify Spotify tracks into two popularity levels (Low and High) based on features such as "danceability," "energy," "valence," etc. The network architecture consisted of three fully connected layers with ReLU activation functions between layers, enabling the model to capture non-linear relationships in the data.

The model was trained over 40 epochs using the Adam optimizer and the cross-entropy loss function. The batch size was set to 32. Hyperparameter tuning was performed to identify the best learning rate.

Results showed that the model achieved a test accuracy of 80.12% when trained with a learning rate of 0.001, making it the best-performing model. The training loss decreased steadily, as seen in the plotted losses, indicating that the model effectively learned the data patterns. However, with larger learning rates (0.01 and 0.1), the model exhibited less stability or poorer convergence, resulting in lower test accuracies of 79.82% and 78.83%, respectively.

**viii. Give examples of hyperparameter tuning that you applied in preparing your project and how you chose the best parameters for models.**

In our project, we performed hyperparameter tuning for the K-Means clustering algorithm by optimizing the number of clusters (k). To determine the best k, we used the elbow method. We tested a range of k values from 1 to 10 and plotted the within-cluster sum of squares (WCSS) against k. The "elbow point," where the decrease in WCSS became less significant, indicated the optimal number of clusters. This approach ensured a balance between model complexity and clustering accuracy, preventing both overfitting (too many clusters) and underfitting (too few clusters).

We also applied hyperparameter tuning to optimize Ridge and Lasso regression models by selecting the best regularization parameter (lambda λ). For Ridge regression, we tested λ values logarithmically spaced between 10^{-1} and 10^6, using 10-fold cross-validation to calculate the negative root mean squared error (rMSE) for each λ. The optimal λ was chosen based on the minimum mean rMSE, and we also applied the 1SE rule to identify a more generalized λ value within one standard error of the minimum RMSE. Similarly, for Lasso regression, we tested λ values from 10^{-1} to 10^4 using the same cross-validation process. The results for both methods ensured an optimal balance between bias and variance, enabling the models to perform well on both training and validation datasets.

The optimal learning rate for the neural network is also a hyperparameter we investigated. Three learning rates were tested—0.001, 0.01, and 0.1—to evaluate their impact on the model's performance. The results showed that a learning rate of 0.001 achieved the highest test accuracy of 80.12%, with a steady decrease in training loss over 40 epochs, indicating effective learning and stability. In contrast, the learning rate of 0.01 produced a slightly lower test accuracy of 79.82%, with slower convergence, while the learning rate of 0.1 resulted in the poorest accuracy of 78.83% and unstable training loss, suggesting that it was too large for the model to converge properly. Based on these observations, 0.001 was chosen as the optimal learning rate, balancing accuracy and training stability. This selection was further validated through visualization, where the training loss curve for 0.001 demonstrated consistent improvement compared to the other rates. 

