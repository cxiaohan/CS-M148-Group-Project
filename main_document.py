# -*- coding: utf-8 -*-
"""Main_LogisticRegression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E2ekqC26UFe1Jxf3Se4mn798PvVS90_Y

# Logistic Regression Pipeline - for Main Document
"""

import pandas as pd
import numpy as np
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.options.mode.copy_on_write = True

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, roc_auc_score
from sklearn.model_selection import KFold, cross_val_score, train_test_split

spotify=pd.read_csv("spotify_clean.csv")

spotify.head()

"""### Distribution of Popularity"""

# Plot histogram for visualization of popularity distribution
plt.figure(figsize=(10, 6))
plt.hist(spotify['popularity'], bins=30, edgecolor='k', alpha=0.7)
plt.axvline(x=50, color='r', linestyle='--', label='Threshold=50')
plt.title('Popularity Distribution')
plt.xlabel('Popularity')
plt.ylabel('Frequency')
plt.legend()
plt.show()

"""Choose a threshold to define whether a song is considered popular or not:
We observed that the distribution is skewed, showing most tracks with lower popularity. We chose 50 to be the threshold because 50 is the natural midpoint of a score scale of 0-100 and for the purpose of interpretability.
"""

popularity_threshold = 50
# encode popularity to binary variables using the threshold
spotify['popularity_binary'] = spotify['popularity'].apply(lambda x: 1 if x > popularity_threshold else 0)

print(spotify['popularity_binary'].value_counts(normalize=True)) # make sure it is not very imbalanced

"""Train Model"""

X = spotify.drop(['popularity_binary', 'popularity', 'track_id', 'artists', 'album_name', 'track_name'], axis=1)
y = spotify['popularity_binary']

# split data, test-train-val
X_train, X_other, y_train, y_other = train_test_split(X, y, test_size=0.4, random_state=4)  # Train: 60%
X_val, X_test, y_val, y_test = train_test_split(X_other, y_other, test_size=0.5, random_state=4)  # Val: 20%, Test: 20%

# fit model
lr_mod = LogisticRegression(max_iter=1000)
lr_mod.fit(X_train, y_train)

# Predict probabilities
y_train_proba = lr_mod.predict_proba(X_train)[:, 1]  # Probabilities for the positive class

# Set threshold to 0.2 since the positive and negative classes are imbalanced in a ratio of 2:8
threshold = 0.2
y_train_pred = (y_train_proba >= threshold).astype(int)  # Convert probabilities to binary predictions

"""### Evaluate model on training set"""

# confusion matrix
conf_matrix = confusion_matrix(y_train, y_train_pred)
tn, fp, fn, tp = conf_matrix.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_train, y_train_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)

# Print results
print("Confusion Matrix:")
print(conf_matrix)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")

# Plot ROC curve to evaluation model performance on validation set
y_prob = lr_mod.predict_proba(X_val)[:, 1]
fpr, tpr, thresholds = roc_curve(y_val, y_prob)
roc_auc = roc_auc_score(y_val, y_prob)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.2f})', color='darkorange')
plt.plot([0, 1], [0, 1], color='navy', linestyle='--')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

"""Based on the ROC curve’s shape, we observe a strong ability for the logistic regression model to differentiate between the two classes, with a relatively high AUC of 0.81. The curve’s deviation from the baseline (dashed blue line) indicates that the model is effective, though not perfect, in balancing true positive and false positive rates.

### Evaluate model on validation set: Cross Validation
"""

# 5-fold CV (this is computed with a default threshold of 0.5)
cross_val_score(lr_mod, X_val, y_val, cv=5, scoring='roc_auc') # auc score

# Perform CV with updated threshold of 0.2
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone
from sklearn import metrics

# Shuffle data before splitting
skfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)
# Set threshold to 0.2
custom_threshold = 0.2
i = 1
for train_index, test_index in skfolds.split(X_val, y_val):
    # Clone the model to ensure fresh training
    clone_lr = clone(lr_mod)
    # Split into training and testing folds
    X_train_folds = X_val.iloc[train_index]
    y_train_folds = y_val.iloc[train_index]
    X_test_fold = X_val.iloc[test_index]
    y_test_fold = y_val.iloc[test_index]

    # Fit the model on the training fold
    clone_lr.fit(X_train_folds, y_train_folds)

    # Predict probabilities
    y_pred_proba = clone_lr.predict_proba(X_test_fold)[:, 1]  # Get probabilities for the positive class

    # Apply the custom threshold to make predictions
    y_pred = (y_pred_proba >= custom_threshold).astype(int)

    # Compute AUC and Accuracy
    auc_sample = metrics.roc_auc_score(y_test_fold, y_pred_proba)  # Use probabilities for AUC
    accuracy_sample = metrics.accuracy_score(y_test_fold, y_pred)  # Use thresholded predictions for accuracy

    # Print fold results
    print('Fold:', i)
    print('AUC:', auc_sample)
    print('Accuracy:', accuracy_sample)

    i += 1

"""Both AUC and accuracy scores are fairly high and consistent across folds, indicating that the model is robust to different splits of the data.

### Evaluate on test set
"""

y_test_proba = lr_mod.predict_proba(X_test)[:, 1]
threshold = 0.2
y_test_pred = (y_test_proba >= threshold).astype(int)  # Convert probabilities to binary predictions

# confusion matrix
conf_matrix_test = confusion_matrix(y_test, y_test_pred)
tn, fp, fn, tp = conf_matrix_test.ravel() # extract values

# compute performance metrics
accuracy = accuracy_score(y_test, y_test_pred)
error = 1 - accuracy
true_positive_rate = tp / (tp + fn)
true_negative_rate = tn / (tn + fp)

# Print results
print("Confusion Matrix:")
print(conf_matrix_test)
print(f"Prediction Accuracy: {accuracy:.2f}")
print(f"Prediction Error: {error:.2f}")
print(f"True Positive Rate (TPR): {true_positive_rate:.2f}")
print(f"True Negative Rate (TNR): {true_negative_rate:.2f}")

"""### Examine model coefficients"""

# Assuming `best_model` is your trained logistic regression model
features = X.columns.tolist()

coefficients = lr_mod.coef_[0]  # For binary classification, get the coefficients for class 1
intercept = lr_mod.intercept_[0]

coef_df = pd.DataFrame({'Feature': features, 'Coefficient': coefficients})
coef_df = coef_df.sort_values(by='Coefficient', ascending=False)

# Print the coefficients
print("Intercept:", intercept)
print("Coefficients:\n", coef_df)

